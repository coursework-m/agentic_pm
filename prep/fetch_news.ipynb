{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38325780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "# Configuration\n",
    "TICKERS = [\"AAPL\", \"GOOG\", \"MSFT\", \"META\", \"TSLA\", \"NVDA\", \"AMZN\", \"NFLX\"]\n",
    "START_DATE = \"2025-01-01\"\n",
    "END_DATE = \"2025-06-30\"\n",
    "OUTPUT_DIR = \"../data/articles\"\n",
    "# Alpha Vantage API key\n",
    "API_KEY = os.environ[\"AV_TOKEN\"]\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "run_date = datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(END_DATE, \"%Y-%m-%d\")\n",
    "print(run_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb211e0-fb7c-43cc-b00e-4dc9519e4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import yfinance as yf\n",
    "\n",
    "# Download historical data\n",
    "hist_data = yf.download(TICKERS, start=START_DATE, end=END_DATE, group_by='ticker', auto_adjust=False)\n",
    "\n",
    "# Cache static info\n",
    "info_cache = {ticker: yf.Ticker(ticker).info for ticker in TICKERS}\n",
    "\n",
    "def fetch_news_sentiment(ticker, start_date, stop_date):\n",
    "    \"\"\"Sentiment\"\"\"\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    print(f\"Fetching news sentiment for {ticker}...\")\n",
    "    params = {\n",
    "        \"function\": \"NEWS_SENTIMENT\",\n",
    "        \"tickers\": ticker,\n",
    "        \"apikey\": API_KEY,\n",
    "        \"limit\": 10000,  # Adjust as needed\n",
    "        \"time_from\": start_date.strftime(\"%Y%m%dT%H%M\"),\n",
    "        \"time_to\": stop_date.strftime(\"%Y%m%dT%H%M\"),\n",
    "        \"sort\": \"EARLIEST\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    data = response.json()\n",
    "    # print(data)\n",
    "\n",
    "    if \"feed\" not in data:\n",
    "        print(f\"No news data found for {ticker}:\", data)\n",
    "        return None\n",
    "\n",
    "    articles = []\n",
    "    for article in data[\"feed\"]:\n",
    "        published_date = article[\"time_published\"][:8]  # Extract YYYYMMDD\n",
    "        sentiment_score = float(article[\"overall_sentiment_score\"])  # Convert to float\n",
    "\n",
    "        # Determine sentiment label based on sentiment score\n",
    "        if sentiment_score <= -0.35:\n",
    "            sentiment_label = \"bearish\"\n",
    "        elif -0.35 < sentiment_score <= -0.15:\n",
    "            sentiment_label = \"somewhat-bearish\"\n",
    "        elif -0.15 < sentiment_score < 0.15:\n",
    "            sentiment_label = \"neutral\"\n",
    "        elif 0.15 <= sentiment_score < 0.35:\n",
    "            sentiment_label = \"somewhat-bullish\"\n",
    "        else:\n",
    "            sentiment_label = \"bullish\"\n",
    "        \n",
    "        article_data = {\n",
    "            \"Ticker\": ticker,\n",
    "            \"Title\": article.get(\"title\", \"\"),\n",
    "            \"Published\": published_date,\n",
    "            \"Sentiment\": sentiment_label.capitalize(),\n",
    "            \"Sentiment Score\": article.get(\"overall_sentiment_score\", \"\"),\n",
    "            \"Summary\": article.get(\"summary\", \"\"),\n",
    "            \"Source\": article.get(\"source\", \"\"),\n",
    "            \"Source_domain\": article[\"source_domain\"],\n",
    "            \"URL\": article.get(\"url\", \"\"),\n",
    "            \"Full_Article\": None\n",
    "        }\n",
    "        articles.append(article_data)\n",
    "        # try:\n",
    "        #     print(f\"Attempting to scrape article {article[\"url\"]}\"\n",
    "        #     article_response = requests.get(article[\"url\"], timeout=10)\n",
    "        #     soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "        #     # Extract the main text content (simplified approach)\n",
    "        #     paragraphs = soup.find_all(\"p\")\n",
    "        #     full_text = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "        #     article_data[\"Full_Article\"] = full_text\n",
    "        #     articles.append(article_data)\n",
    "        # except ConnectionError as e:\n",
    "        #     article_data[\"Full_Article\"] = f\"Error fetching article: {e}\"\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aff71b-7618-4e88-ae27-a4bbf99011e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize ticker_last_run to track the last run date for each ticker\n",
    "ticker_next_run = {ticker: run_date for ticker in TICKERS}\n",
    "try:\n",
    "    # Loop through each ticker and fetch news sentiment\n",
    "    for ticker in TICKERS:\n",
    "        run_date = ticker_next_run[ticker]\n",
    "        while run_date <= end_date:\n",
    "            print(f\"Processing date: {run_date.strftime('%Y-%m-%d')}\")\n",
    "            # Fetch news sentiment for each ticker\n",
    "            articles = fetch_news_sentiment(ticker, run_date, end_date)\n",
    "            pprint(articles)\n",
    "            ticker_next_run[ticker] += timedelta(days=1)\n",
    "            file = ticker + \"_\" + run_date.strftime('%Y-%m-%d') + \"_\" + end_date.strftime('%Y-%m-%d')\n",
    "            filename = os.path.join(OUTPUT_DIR, f\"{file}.json\")\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(articles, f, indent=2)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198184a-ae55-4edc-8ce7-c501dacfb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aec7e4-b6cd-4982-bb07-1f529477dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_next_run[ticker] += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ticker_next_run[ticker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad1792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e9ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from alpha_vantage_pro import AlphaVantageClient, AlphaVantageClientAsync\n",
    "run_date = datetime.strptime(START_DATE, \"%Y-%m-%d\").strftime(\"%Y%m%dT%H%M\")\n",
    "end_date = datetime.strptime(END_DATE, \"%Y-%m-%d\").strftime(\"%Y%m%dT%H%M\")\n",
    "TICKERS = [\"AAPL\", \"GOOG\", \"MSFT\", \"META\", \"TSLA\", \"NVDA\", \"AMZN\", \"NFLX\"]\n",
    "print(run_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b209b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AlphaVantageClient(api_key=os.environ['AV_TOKEN'])\n",
    "df = client.get_daily(\"AAPL\")\n",
    "news = client.get_news_sentiment(TICKERS, run_date, end_date, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news.count())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6065c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AlphaVantageClientAsync(api_key=\"YOUR_KEY\")\n",
    "df = await client.get_daily_async(TICKERS)\n",
    "await client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51837bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e8eea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
